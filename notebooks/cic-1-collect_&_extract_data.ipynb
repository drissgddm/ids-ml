{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to download data from https://www.unb.ca/cic/datasets/ids-2018.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's see first what's in the bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --no-sign-request \"s3://cse-cic-ids2018\" --recursive --human-readable --summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now download only the csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --no-sign-request \"s3://cse-cic-ids2018/Processed Traffic Data for ML Algorithms/\" data/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l --block-size=M data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates_by_datasets = {\n",
    "#     \"Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\": \"02-03-2018\",\n",
    "#     \"Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\": \"16-02-2018\",\n",
    "#     \"Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\": \"23-02-2018\",\n",
    "#     \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv\": \"23-02-2018\",\n",
    "#     \"Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\": \"01-03-2018\",\n",
    "#     \"Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\": \"15-02-2018\",\n",
    "#     \"Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\": \"22-02-2018\",\n",
    "#     \"Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\": \"14-02-2018\",\n",
    "#     \"Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\": \"21-02-2018\",\n",
    "#     \"Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\": \"28-02-2018\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that dataset \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv\" is too heavy\n",
    "\n",
    "Let's split it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huge_df = pd.read_csv(\"data/Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_chunks = 10\n",
    "start, chunk_size = 0, int(huge_df.shape[0] / nb_of_chunks)\n",
    "\n",
    "for chunk in range(nb_of_chunks):\n",
    "    huge_df.iloc[start: start + (chunk_size)].to_csv(\n",
    "        'data/Thuesday-20-02-2018_TrafficForML_CICFlowMeter_{}.csv'.format(chunk),\n",
    "        index=False\n",
    "    )\n",
    "    start += chunk_size\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l --block-size=M data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data/Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [\n",
    "    \"Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter_0.csv\",\n",
    "    \"Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter_1.csv\",\n",
    "    \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter_2.csv\",\n",
    "    \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter_3.csv\",\n",
    "    \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter_4.csv\",\n",
    "    \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter_5.csv\",\n",
    "    \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter_6.csv\",\n",
    "    \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter_7.csv\",\n",
    "    \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter_8.csv\",\n",
    "    \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter_9.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore labels distribution and memory usage for each datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dist_by_dataset, tot_mem = {}, 0\n",
    "\n",
    "for csv in csv_files:\n",
    "\n",
    "    print('Loading', csv)\n",
    "    tmp_df = pd.read_csv(\"data/{}\".format(csv))\n",
    "    tmp_dist = (tmp_df['Label'].value_counts(normalize=True) * 100).to_dict()\n",
    "    tmp_mem = tmp_df.memory_usage(index=True).sum()\n",
    "    tot_mem += tmp_mem\n",
    "    tmp_dist['memory_usage'] = tmp_mem\n",
    "    print(tmp_dist)\n",
    "    labels_dist_by_dataset[csv] = tmp_dist\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize memory usage to be more understandable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in labels_dist_by_dataset:\n",
    "    labels_dist_by_dataset[d]['memory_usage'] = round((labels_dist_by_dataset[d]['memory_usage'] / tot_mem) * 100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go! Now having more representative stats on each dataset for splitting into train, test and validate sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dist_by_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huge_df['Label'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print columns the names of the first columns because I noticed some differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv in csv_files:\n",
    "\n",
    "    print('Loading', csv)\n",
    "    tmp_df = pd.read_csv(\"data/{}\".format(csv))\n",
    "    print(tmp_df.columns[:5])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the datasets are unbalanced. We must therefore find a compromise to have enough \"M-Profile\" labeled data (malicious) of each datasets in the training set while avoiding any process of false data generation such as oversampling or something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_columns(df):\n",
    "    df.columns = [col.upper().replace(' ', '_') for col in df.columns]\n",
    "    df = df.drop(['FLOW_ID', 'SRC_IP', 'DST_IP', 'SRC_PORT'], axis=1, errors='ignore')\n",
    "    # ...\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_test_validation_split(df, test_size, val_size, label_column='LABEL'):\n",
    "    '''\n",
    "        :return: train_dataset (pandas.DataFrame), \n",
    "                 test_dataset (pandas.DataFrame), \n",
    "                 validation_dataset (pandas.DataFrame)\n",
    "    '''\n",
    "    # Format columns\n",
    "    df = format_columns(df)\n",
    "    \n",
    "    # Splitting X and y from df\n",
    "    X, y = df[df.columns.difference(['LABEL'])], df['LABEL']\n",
    "    # Cutting out test set from df\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=1)\n",
    "    # Cutting out val set from train_df\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=1)\n",
    "    \n",
    "    return pd.concat([X_train, y_train], axis=1), pd.concat([X_test, y_test], axis=1), pd.concat([X_val, y_val], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's keep this files list for creating our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [\n",
    "    \"Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter_0.csv\",\n",
    "    \"Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "    \"Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\",\n",
    "#     \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter_1.csv\",\n",
    "#     \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter_2.csv\",\n",
    "#     \"Thuesday-20-02-2018_TrafficForML_CICFlowMeter_3.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Loading Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\n",
      "Train set concatenated: new train dataframe shape: 631241\n",
      "Test set concatenated: new test dataframe shape: 314573\n",
      "Val set concatenated: new test dataframe shape: 102761\n",
      "*** Loading Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Driss\\miniconda3\\envs\\ids-ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set concatenated: new train dataframe shape: 1262482\n",
      "Test set concatenated: new test dataframe shape: 629146\n",
      "Val set concatenated: new test dataframe shape: 205522\n",
      "*** Loading Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cae8b8b27648>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     tmp_train, tmp_test, tmp_val = train_test_validation_split(\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mval_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# (1 - 0.3) * 0.14 = 0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ids-ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ids-ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ids-ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ids-ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1043\u001b[0m             )\n\u001b[0;32m   1044\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1045\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ids-ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1863\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1864\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ids-ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1355\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m         \"\"\"\n\u001b[1;32m-> 1357\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1358\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ids-ml\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    640\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv'"
     ]
    }
   ],
   "source": [
    "train, test, val = pd.DataFrame({}), pd.DataFrame({}), pd.DataFrame({})\n",
    "\n",
    "for csv in csv_files:\n",
    "\n",
    "    print('*** Loading', csv)\n",
    "    \n",
    "    tmp_train, tmp_test, tmp_val = train_test_validation_split(\n",
    "        df=pd.read_csv(\"data/{}\".format(csv)),\n",
    "        test_size=0.3,\n",
    "        val_size=0.14, # (1 - 0.3) * 0.14 = 0.1\n",
    "    )\n",
    "    \n",
    "    \n",
    "    train = pd.concat([train, tmp_train])\n",
    "    print('Train set concatenated: new train dataframe shape: {}'.format(train.shape[0]))\n",
    "    test = pd.concat([test, tmp_test])\n",
    "    print('Test set concatenated: new test dataframe shape: {}'.format(test.shape[0]))\n",
    "    val = pd.concat([val, tmp_val])\n",
    "    print('Val set concatenated: new test dataframe shape: {}'.format(val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val.shape)\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train.columns) == list(test.columns) == list(val.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, export them in csv files for an upcoming exploration, modelization, test and validation...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.to_csv('val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
